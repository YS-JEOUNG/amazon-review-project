{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "artscrafts_bow.ipynb",
      "provenance": [],
      "mount_file_id": "1-oDlahVJIRhJW0ckqeDr-MoLdEhjDzB3",
      "authorship_tag": "ABX9TyO+I0uBpEyhTQ4yXVum8XXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YS-JEOUNG/amazon-review-project/blob/main/code/artscrafts_code/artscrafts_bow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxA_FXBB1tzt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JezRoxF1rgo",
        "outputId": "9256dc8c-da22-49b0-9629-ee8c6463a249"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize, regexp_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJvzgScx2rsj",
        "outputId": "62f81140-cd39-4de2-fe3e-e76ae4995b46"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 4.3MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 16.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85389 sha256=149b3f25db3f4da552f743414feb7bb833e75ecd07442df4e122a4f8c24a60e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYeK-BAh2wWo"
      },
      "source": [
        "from contractions import contractions_dict     # CONTRACTION_MAP 대신 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Sb8xad2-oc"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcTROS8v2_6d"
      },
      "source": [
        "total = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/project/arts_total.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFhds9VW25DU"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJSZ8Z5t21ue",
        "outputId": "f6b9060c-1f0d-427f-deaa-a8b335b4cf7e"
      },
      "source": [
        "# calculate raw tokens in order to measure of cleaned tokens\n",
        "raw_tokens = len([w for t in (total['review_text'].apply(word_tokenize)) for w in t])\n",
        "print(f'number of raw tokens: {raw_tokens}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of raw tokens: 11737017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KDDpGSjWUxg",
        "outputId": "b6521e5c-45e3-459d-a97f-3af3e4d6fd47"
      },
      "source": [
        "total['review_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Nice and perfect for an address book Thin, you...\n",
              "1         Solid buy, few small gripes about page layout ...\n",
              "2         Cool place to Catalog Your Recipes I purchased...\n",
              "3         Its ok read for specifications I did not like ...\n",
              "4         Great for Chefs. I'm a chef and I was looking ...\n",
              "                                ...                        \n",
              "130128     Five Stars Awesome set to have for any croceter.\n",
              "130129    Variations in thickness with various colors on...\n",
              "130130    super happy! Is really small but that's what I...\n",
              "130131    Most excellent quality sock yarn on the market...\n",
              "130132    a yarn that makes perfect socks. I finished 2 ...\n",
              "Name: review_text, Length: 130133, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nY20ii0DQB-"
      },
      "source": [
        "def strip_html(text):\n",
        "  # html코드 제거 함수\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "  # 괄호로 묶여있는 단어 제거\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "  # 위에 함수 두개 과정 진행하는 함수\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Define function to expand contractions\n",
        "def expand_contractions(text):\n",
        "  # IGNORECASE =  대소문자 관련 없이, DOTALL = 줄바꿈기호 상관없이 매칭해달라는 의미\n",
        "  # re를 이용하여 정규표현식으로 매칭을 할 때 contractions_dict의 키를   \n",
        "  # 찾고자 하는 형태를 만든뒤 매칭하여 축약문들을 확장 시키는 과정인듯\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contractions_dict.get(match)\\\n",
        "                        if contractions_dict.get(match)\\\n",
        "                        else contractions_dict.get(match.lower())\n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "    \n",
        "    try:\n",
        "      expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "      expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    except:\n",
        "      return text\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "# special_characters removal\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "  # 문자, 숫자가 아닌것들을 제거\n",
        "  # r은 정규 표현식을 쓸 때 가독성을 위한 장치 raw string이라는 의미\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def remove_punctuation_and_splchars(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_word = remove_special_characters(new_word, True)\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "stopword_list= stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "#stopword_list.remove('headphone')\n",
        "#stopword_list.remove('headphones')\n",
        "#stopword_list.remove('earbuds')\n",
        "#stopword_list.remove('bud')\n",
        "#stopword_list.remove('ear')\n",
        "#stopword_list.remove('sony')\n",
        "#stopword_list.remove('product')\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopword_list:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation_and_splchars(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "def lemmatize(words):\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfv_TgDDghW"
      },
      "source": [
        "def normalize_and_lemmatize(input):\n",
        "    sample = denoise_text(input)\n",
        "    sample = expand_contractions(sample)\n",
        "    sample = remove_special_characters(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = normalize(words)\n",
        "    lemmas = lemmatize(words)\n",
        "    return ' '.join(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElT53oOoDho-"
      },
      "source": [
        "total['clean_text'] = total['review_text'].map(lambda text: normalize_and_lemmatize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-82hopYFczU",
        "outputId": "5a2e60cf-c36a-4a55-a54d-76b50b77cb61"
      },
      "source": [
        "total['clean_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         nice perfect address book thin could slide dra...\n",
              "1         solid buy small grip page layout honest alread...\n",
              "2         cool place catalog recipes purchase recipe jou...\n",
              "3         ok read specifications not like really like mo...\n",
              "4         great chefs chef look book jot quick ideas ins...\n",
              "                                ...                        \n",
              "130128                       five star awesome set croceter\n",
              "130129    variations thickness various color x little lo...\n",
              "130130            super happy really small look super happy\n",
              "130131    excellent quality sock yarn market use regia s...\n",
              "130132    yarn make perfect sock finish pair sock years ...\n",
              "Name: clean_text, Length: 130133, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu_kdNvkFfxU",
        "outputId": "3ec4554a-5000-4f97-e2cc-937c8b2712a5"
      },
      "source": [
        "total.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 130133 entries, 0 to 130132\n",
            "Data columns (total 20 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   overall           130133 non-null  float64\n",
            " 1   vote              130133 non-null  int64  \n",
            " 2   verified          130133 non-null  bool   \n",
            " 3   reviewer_id       130133 non-null  object \n",
            " 4   asin              130133 non-null  object \n",
            " 5   style             130133 non-null  object \n",
            " 6   reviewer_name     130128 non-null  object \n",
            " 7   unix_review_time  130133 non-null  int64  \n",
            " 8   category          130133 non-null  object \n",
            " 9   description       130133 non-null  object \n",
            " 10  title             130133 non-null  object \n",
            " 11  brand             129770 non-null  object \n",
            " 12  rank              130133 non-null  object \n",
            " 13  main_cat          129791 non-null  object \n",
            " 14  date              20908 non-null   object \n",
            " 15  price             115105 non-null  object \n",
            " 16  review_text       130133 non-null  object \n",
            " 17  rating_class      130133 non-null  object \n",
            " 18  time              130133 non-null  object \n",
            " 19  clean_text        130133 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(2), object(16)\n",
            "memory usage: 19.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z2NniqpFkad",
        "outputId": "b7973115-5c46-43dd-9113-643abe1d35f2"
      },
      "source": [
        "# cleaning the text\n",
        "clean_tokens = len([w for t in (total['clean_text'].apply(word_tokenize)) for w in t])\n",
        "print(f'number of clean tokens: {clean_tokens}')\n",
        "print(f'percentage of removed tokens: {1-(clean_tokens/raw_tokens)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of clean tokens: 5260371\n",
            "percentage of removed tokens: 0.5518136337367493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8uKiSbSGExu"
      },
      "source": [
        "total.to_csv('/content/drive/MyDrive/Colab Notebooks/project/arts_total_cleaned.csv', sep=',', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}